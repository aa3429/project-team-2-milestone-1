{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVrlfUfi0ngbcD81zPamC5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aa3429/project-team-two-milestone-four/blob/main/Milestone_5_Documentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning for Custom Datasets in the Small-Data Regime\n"
      ],
      "metadata": {
        "id": "b9iJQkUEQ-Dm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Milestone 1: In this stage you will be working with CVAT - you need to install CVAT locally to your laptop/desktop"
      ],
      "metadata": {
        "id": "NKo2VekvVVvD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation Instructions (Windows and MacOS)\n",
        "\n",
        "### Windows 10:\n",
        "\n",
        "1) Install WSL2 (Windows subsystem for Linux) refer to this official guide: \"https://docs.microsoft.com/windows/wsl/install-win10\". WSL2 requires Windows 10, version 2004 or higher. Note: You may not have to install a Linux distribution unless needed.\n",
        "\n",
        "2) Download and install Docker Desktop for Windows:\"https://download.docker.com/win/stable/Docker%20Desktop%20Installer.exe\". Double-click Docker for Windows Installer to run the installer. More instructions can be found here:\"https://docs.docker.com/docker-for-windows/install/\". Official guide for docker WSL2 backend can be found here:\"https://docs.docker.com/docker-for-windows/wsl/\". Note: Check that you are specifically using WSL2 backend for Docker.\n",
        "\n",
        "3) Download and install Git for Windows:\"https://github.com/git-for-windows/git/releases/download/v2.21.0.windows.1/Git-2.21.0-64-bit.exe\". When installing the package please keep all options by default. More information about the package can be found here:\"https://gitforwindows.org/\".\n",
        "\n",
        "4) Download and install Google Chrome:\"https://www.google.com/chrome/\". It is the only browser which is supported by CVAT.\n",
        "\n",
        "5) Go to windows menu, find Git Bash application and run it. You should see a terminal window.\n",
        "\n",
        "6) Clone CVAT source code from the GitHub repository:\"https://github.com/opencv/cvat\".\n",
        "\n",
        "7) The following command will clone the latest develop branch:\n",
        "\n",
        "git clone https://github.com/opencv/cvat\n",
        "cd cvat\n",
        "\n",
        "See alternatives:\"https://opencv.github.io/cvat/docs/administration/basics/installation/#how-to-get-cvat-source-code\" if you want to download one of the release versions.\n",
        "\n",
        "8) Run docker containers. It will take some time to download the latest CVAT release and other required images like postgres, redis, etc. from DockerHub and create containers.\n",
        "\n",
        "docker-compose up -d\n",
        "\n",
        "(Optional) Use CVAT_VERSION environment variable to specify the version of CVAT you want to install specific version (e.g v2.1.0, dev). Default behavior: dev images will be pulled for develop branch, and corresponding release images for release versions.\n",
        "\n",
        "9) CVAT_VERSION=dev docker-compose up -d\n",
        "\n",
        "Alternative: if you want to build the images locally with unreleased changes see How to pull/build/update CVAT images section:\"https://opencv.github.io/cvat/docs/administration/basics/installation/#how-to-pullbuildupdate-cvat-images\"\n",
        "\n",
        "You can register a user but by default it will not have rights even to view list of tasks. Thus you should create a superuser. A superuser can use an admin panel to assign correct groups to other users. Please use the command below:\n",
        "\n",
        "10) winpty docker exec -it cvat_server bash -ic 'python3 ~/manage.py createsuperuser'\n",
        "\n",
        "11) If you don’t have winpty installed or the above command does not work, you may also try the following:\n",
        "\n",
        "enter docker image first\n",
        "docker exec -it cvat_server /bin/bash\n",
        "then run\n",
        "python3 ~/manage.py createsuperuser\n",
        "\n",
        "Choose a username and a password for your admin account. For more information please read Django documentation:\"https://docs.djangoproject.com/en/2.2/ref/django-admin/#createsuperuser\".\n",
        "\n",
        "12) Open the installed Google Chrome browser and go to localhost:8080 \"http://localhost:8080/\". Type your login/password for the superuser on the login page and press the Login button. Now you should be able to create a new annotation task. Please read the CVAT manual:\"https://opencv.github.io/cvat/docs/manual/\" for more details.\n",
        "\n",
        "\n",
        "\n",
        "### Mac OS Mojave\n",
        "\n",
        "1) Download Docker for Mac:\"https://download.docker.com/mac/stable/Docker.dmg\". Double-click Docker.dmg to open the installer, then drag Moby the whale to the Applications folder. Double-click Docker.app in the Applications folder to start Docker. More instructions can be found here:\"https://docs.docker.com/v17.12/docker-for-mac/install/#install-and-run-docker-for-mac\".\n",
        "\n",
        "2) There are several ways to install Git on a Mac. The easiest is probably to install the Xcode Command Line Tools. On Mavericks (10.9) or above you can do this simply by trying to run git from the Terminal the very first time.\n",
        "\n",
        "3) git --version\n",
        "\n",
        "If you don’t have it installed already, it will prompt you to install it. More instructions can be found here:\"https://git-scm.com/book/en/v2/Getting-Started-Installing-Git\".\n",
        "\n",
        "Download and install Google Chrome:\"https://www.google.com/chrome/\". It is the only browser which is supported by CVAT.\n",
        "\n",
        "4) Open a terminal window. The terminal app is in the Utilities folder in Applications. To open it, either open your Applications folder, then open Utilities and double-click on Terminal, or press Command - spacebar to launch Spotlight and type “Terminal,” then double-click the search result.\n",
        "\n",
        "5) Clone CVAT source code from the GitHub repository:\"https://github.com/opencv/cvat\" with Git.\n",
        "\n",
        "The following command will clone the latest develop branch:\n",
        "\n",
        "git clone https://github.com/opencv/cvat\n",
        "cd cvat\n",
        "\n",
        "See alternatives:\"https://opencv.github.io/cvat/docs/administration/basics/installation/#how-to-get-cvat-source-code\" if you want to download one of the release versions or use the wget or curl tools.\n",
        "\n",
        "6) Run docker containers. It will take some time to download the latest CVAT release and other required images like postgres, redis, etc. from DockerHub and create containers.\n",
        "\n",
        "7) docker-compose up -d\n",
        "\n",
        "(Optional) Use CVAT_VERSION environment variable to specify the version of CVAT you want to install specific version (e.g v2.1.0, dev). Default behavior: dev images will be pulled for develop branch, and corresponding release images for release versions.\n",
        "\n",
        "8) CVAT_VERSION=dev docker-compose up -d\n",
        "\n",
        "Alternative: if you want to build the images locally with unreleased changes see How to pull/build/update CVAT images section:\"https://opencv.github.io/cvat/docs/administration/basics/installation/#how-to-pullbuildupdate-cvat-images\"\n",
        "\n",
        "You can register a user but by default it will not have rights even to view list of tasks. Thus you should create a superuser. A superuser can use an admin panel to assign correct groups to other users. Please use the command below:\n",
        "\n",
        "9) docker exec -it cvat_server bash -ic 'python3 ~/manage.py createsuperuser'\n",
        "\n",
        "Choose a username and a password for your admin account. For more information please read Django documentation:\"https://docs.djangoproject.com/en/2.2/ref/django-admin/#createsuperuser\".\n",
        "\n",
        "10) Open the installed Google Chrome browser and go to localhost:8080 (\"http://localhost:8080/\"). Type your login/password for the superuser on the login page and press the Login button. Now you should be able to create a new annotation task. Please read the CVAT manual:\"https://opencv.github.io/cvat/docs/manual/\" for more details."
      ],
      "metadata": {
        "id": "DCSM-4i0XGnr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Milestone 2: Data Acquisition\n",
        "Below is the code used to achieve the image scraping task\n",
        "\n",
        "#####To make this code work: Replace the desired URL, and give a folder name in the function call line for imagedown function and run it in Colab, images will be downloaded in the Colab local folder or PC local folder (if executed with local IDE)\n",
        "#####Colab Link for PyTest Substitute - https://colab.research.google.com/drive/1wC2YLKvmoxn6J1t_DKLot08GxbQozz6u?usp=sharing\n",
        "\n",
        "Code is present in the next block\n",
        "##### Replace URL and folder name in the function call line for imagedown function above"
      ],
      "metadata": {
        "id": "Zo3faRbPYrZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        " \n",
        "def imagedown(url, folder):\n",
        "    try:\n",
        "        os.mkdir(os.path.join(os.getcwd(), folder))\n",
        "    except:\n",
        "        pass\n",
        "    os.chdir(os.path.join(os.getcwd(), folder))\n",
        "    #r = requests.get(url)\n",
        "    r = urlopen(url)\n",
        "    #soup = BeautifulSoup(r.text, 'html.parser')\n",
        "    soup = BeautifulSoup(r, 'html.parser')\n",
        "    #images = soup.find_all('img')\n",
        "    images = soup.find_all('img', width=300, height=300)\n",
        "    i=0\n",
        "    for image in images:\n",
        "        #name = str(i) + 'image'\n",
        "        suffixstr = str(i)\n",
        "        i+=1\n",
        "        name = image['alt']\n",
        "        link = image['src']\n",
        "        #with open(name + '.jpg', 'wb') as f:\n",
        "        with open(name.replace(' ', '-').replace('/', '').replace('.', '') + suffixstr + '.jpg', 'wb') as f:\n",
        "            im = requests.get(link)\n",
        "            f.write(im.content)\n",
        "            print('Writing: ', name)\n",
        " \n",
        "imagedown('https://www.homedepot.com/b/Bath-Bathroom-Faucets-Bathroom-Sink-Faucets-Centerset-Bathroom-Faucets/N-5yc1vZbrhk?Nao=48','faucet1')\n",
        "# Replace URL and folder name in the function call line for imagedown function above"
      ],
      "metadata": {
        "id": "0_V0ilpKagMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Milestone 3: Annotation\n",
        "\n",
        "Link to annotated images: https://drive.google.com/file/d/13636lJOiOi_fiqt0yk_E2daNccDDQfEP/view?usp=sharing\n",
        "https://drive.google.com/file/d/1MTPtINTizdFDVqI6h4duVT4xVEV8phEY/view?usp=sharing\n",
        "\n",
        "DEXTR, or Deep Extreme Cut, obtains an object segmentation from its four extreme points: the left-most, right-most, top, and bottom pixels. The annotated extreme points are given as a guiding signal to the input of the network. To this end, we create a heatmap with activations in the regions of extreme points. We center a 2D Gaussian around each of the points, to create a single heatmap. The heatmap is concatenated with the RGB channels of the input image, to form a 4-channel input for the CNN. To focus on the object of interest, the input is cropped by the bounding box, which is a polygon, formed from the extreme point annotations. To include context on the resulting crop, we relax the tight bounding box by several pixels. After the pre-processing step that comes exclusively from the extreme clicks, the input consists of an RGB crop including an object, plus its extreme points.\n",
        "\n",
        "The user provides the extreme clicks for an object, and the CNN produces the segmented masks. DEXTR can generate high-quality class-agnostic masks given only extreme points as input. The resulting masks can in turn be used to train other deep architectures for other tasks or datasets, that is, we use extreme points to annotate a new dataset with object segmentations.\n",
        "\n",
        "ResNet-101 is chosen as backbone of the architecture. We remove the fully connected layers as well as the max pooling layers in the last two stages to preserve acceptable output resolution for dense prediction, and we introduce atrous convolutions in the last two stages to maintain the same receptive field. After the last ResNet-101 stage, we introduce a pyramid scene parsing module to aggregate global context to the final feature map. The output of the CNN is a probability map representing whether a pixel belongs to the object that we want to segment or not. The CNN is trained to minimize the standard cross entropy loss, which considers that different classes occur with different frequency in a dataset.\n",
        "\n",
        "Usage for DEXTR:\n",
        "i)\tClass-agnostic Instance Segmentation: One application of DEXTR is class-agnostic instance segmentation. In this task, we click on the extreme points of an object in an image, and we obtain a mask prediction for it. The selected object can be of any class, as the method is class agnostic.\n",
        "ii)\tAnnotation: The common annotation pipeline for segmentation can also be assisted by DEXTR. In this framework, instead of detailed polygon labels, the workload of the annotator is reduced to only providing the extreme points of an object, and DEXTR produces the desired segmentation.\n",
        "iii)\tVideo Object Segmentation: DEXTR can also improve the pipeline of video object segmentation. We focus on the semi-supervised setting where methods use one or more masks as inputs to produce the segmentation of the whole video.\n",
        "\n",
        "For Annotation: DEXTR can generate high-quality class-agnostic masks given only extreme points as input. The resulting masks can in turn be used to train other deep architectures for other tasks or datasets, that is, we use extreme points to annotate a new dataset with object segmentations. In our use case, we compare the results of a semantic segmentation algorithm trained on either the ground-truth masks or those generated by DEXTR (we combine all per-instance segmentations into a per-pixel semantic classification result).\n",
        "\n",
        "CVat is an extensive and completely free open-source platform centered around 2D annotations for both images and video.\n",
        "The UI is noticeably less intuitive at times. For instance, setting a filter only hides other annotations from the image being looked at. Unless you know this, there is no indication that you can also cycle through images that contain the filtered object using some key bindings. There is no visual feedback – such as a list of thumbnails – about the filter.\n",
        "\n",
        "For semi-automatic annotation, DEXTR is provided. It works by selecting some extreme points of an object, after which the complete segmentation is derived.\n",
        "\n",
        "DEXTR can also be used to obtain dense annotations to train supervised techniques. It is shows that very accurate annotations are obtained with respect to the ground truth, but more importantly, that algorithms trained on the annotations obtained by DEXTR algorithm perform as good as when trained from the ground-truth ones. If costs are added to obtain such annotations into the equation, then training using DEXTR is significantly more efficient than training from the ground truth for a given target quality.\n",
        "\n",
        "One of the most common ways to perform weakly supervised segmentation is drawing a bounding box around the object of interest. However, to draw the corners of a bounding box, the user must click points outside the object, drag the box diagonally, and ad- just it several times to obtain a tight, accurate bounding box. This process is cognitively demanding, with increased error rates and labelling times.\n",
        "Recently, enhancements have shown a much more efficient way of obtaining a bounding box using extreme clicks, spending on average 7.2 seconds instead of 34.5 seconds required for drawing a bounding box around an object. They show that extreme clicking leads to high quality bounding boxes that are on par with the ones obtained by traditional methods. These extreme points be- long to the top, bottom, left-most and right-most parts of the object. Extreme-clicking annotations provide more information than a bounding box; they contain four points that are on the boundary of the object, from which one can easily obtain the bounding-box. We use extreme points for object segmentation leveraging their two main outcomes: the points and their inferred bounding box.\n",
        "\n",
        "Having the extreme points annotated allows for focusing on specific regions in an image, cropped by the limits specified by them. It is compared how beneficial it is to focus on the region of interest, rather than processing the entire image. To this end, we crop the region surrounded by the extreme points, relaxing it by 50 pixels for increased context and compare it against the full image case. This is because cropping eliminates the scale variation on the input.\n",
        "\n",
        "Thus, DEXTR can also be used as an accurate and efficient mask annotation tool, reducing labeling costs by a factor of 10."
      ],
      "metadata": {
        "id": "zghALEGHbc1v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Milestone 4: Semantic Segmentation"
      ],
      "metadata": {
        "id": "6GaL2lGBcN7X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Code for segmentation"
      ],
      "metadata": {
        "id": "Xtt9nYd9lPJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the libraries\n",
        " \n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import cv2\n",
        "from scipy import io\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "YALCYIlhcVYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/aa3429/project-team-two-milestone-four.git"
      ],
      "metadata": {
        "id": "dQkZlaGQlSe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the cloned repository we will be working with\n",
        "!ls project-team-two-milestone-four/"
      ],
      "metadata": {
        "id": "oDDxcactlZ-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we'll first upload all the images to our local cloud storage or google drive in roder to make this code run\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "items = os.listdir('/content')\n",
        "print (items)"
      ],
      "metadata": {
        "id": "rad5s_Y-ldJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "images=[]\n",
        "for each_image in items:\n",
        "  if each_image.endswith(\".jpg\"):\n",
        "    print (each_image)\n",
        "    full_path = \"/content/\" + each_image\n",
        "    print (full_path)\n",
        "    image = cv2.imread(full_path)\n",
        "    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
        "    images.append(tf.convert_to_tensor(image))"
      ],
      "metadata": {
        "id": "eLOrVJa8lfM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(images)"
      ],
      "metadata": {
        "id": "wQfFZZ-SlkUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bringing the images by clonning the git repository\n",
        "#/content/0183514226.jpg\n",
        "images = []\n",
        "#from PIL import Image\n",
        "#for i in pwd:\n",
        "#image = Image.open('./project-team-two-milestone-four/images/%.jpg')\n",
        "#img=image.show()\n",
        "#img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "#images.append(tf.convert_to_tensor(img)) \n",
        "\n",
        "\n",
        "for i in range(1,1001):\n",
        " url = './clothing-co-parsing/photos/%04d.jpg'%(i)\n",
        " img = cv2.imread(url)\n",
        " img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        " images.append(tf.convert_to_tensor(img))"
      ],
      "metadata": {
        "id": "dY47MkfNllxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Bringing the annotations by clonning the git repository.\n",
        "# Here we'll bring all annotated files of annotations to our google cloud before running the following code.\n",
        "\n",
        "items = os.listdir('/content')\n",
        "print (items)"
      ],
      "metadata": {
        "id": "0zRbQTCzlnKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting Annotations out\n",
        "\n",
        "annotations=[]\n",
        "for each_annot in items:\n",
        "  if each_annot.endswith(\".txt\"):\n",
        "    print (each_annot)\n",
        "    full_path = \"/content/\" + each_annot\n",
        "    with open(full_path) as f:\n",
        "      annots = f.read()\n",
        "      print(annots)\n",
        "      annotations.append(annots)"
      ],
      "metadata": {
        "id": "LrQ1USqFlq8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(annotations)"
      ],
      "metadata": {
        "id": "baTH3-7EluXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(images), len(annotations)"
      ],
      "metadata": {
        "id": "SxAxzp-xluxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing some of the images\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "for i in range(1,4):\n",
        "    plt.subplot(1,3,i)\n",
        "    img = images[i]\n",
        "    plt.imshow(img, cmap='jet')\n",
        "    plt.colorbar()\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dWzek-BblwXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pfGstUGDl8mI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Selection"
      ],
      "metadata": {
        "id": "7L9Bz0_J1UT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will choose Unet as our model and will import its pipelines from a pretraied model. UNet architecture has got many models like - MobileNetV2, ResNet, NASNet, Inception, DenseNet, or EfficientNet."
      ],
      "metadata": {
        "id": "coyKdVeUmCZv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " DenseNet is the pretrained model we are deciding to go with."
      ],
      "metadata": {
        "id": "5OAqC0WGmG7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We prefer a pre-trained DenseNet121 to be the downstack that can be obtained through transfer learning and build the upstack with https://www.tensorflow.org/tutorials/generative/pix2pix, a publicly available generative upstack template (it saves our time and code)."
      ],
      "metadata": {
        "id": "Cxueb_pcmJ1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Build Downstack Model"
      ],
      "metadata": {
        "id": "C4P3P57wmVqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load DenseNet121 from in-built applications.\n",
        "\n",
        "\n",
        "base = keras.applications.DenseNet121(input_shape=[128,128,3], \n",
        "                                       include_top=False, \n",
        "                                       weights='imagenet') \n"
      ],
      "metadata": {
        "id": "v8pLiRd0mO-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the layers\n",
        "\n",
        "len(base.layers)"
      ],
      "metadata": {
        "id": "49yTcY2CmQih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The DenseNet121 model has 427 layers. We need to identify suitable layers whose output will be used for skip connections. "
      ],
      "metadata": {
        "id": "OO7eVxY6mTVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets plot the entire model, along with the feature shapes\n",
        "\n",
        "keras.utils.plot_model(base, show_shapes=True)"
      ],
      "metadata": {
        "id": "FapRNzaQmYc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We select the final ReLU activation layer for each feature map size, i.e. 4, 8, 16, 32, and 64, required for skip-connections. Write down the names of the selected ReLU layers in a list."
      ],
      "metadata": {
        "id": "fEj0XYU9ma5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "skip_names = ['conv1/relu', # size 64*64\n",
        "              'pool2_relu',  # size 32*32\n",
        "              'pool3_relu',  # size 16*16\n",
        "              'pool4_relu',  # size 8*8\n",
        "              'relu'        # size 4*4\n",
        "              ] "
      ],
      "metadata": {
        "id": "dpAIkwBCmcsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain the outputs of these layers.\n",
        "\n",
        "skip_outputs = [base.get_layer(name).output for name in skip_names]"
      ],
      "metadata": {
        "id": "tosNhencmd67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(skip_outputs)):\n",
        "    print(skip_outputs[i]) "
      ],
      "metadata": {
        "id": "o5WDuF1cmf1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the downstack\n",
        "\n",
        "downstack = keras.Model(inputs=base.input,\n",
        "                        outputs=skip_outputs)\n",
        "downstack.trainable = False"
      ],
      "metadata": {
        "id": "jLK0JBkymhQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Build Upstack Model"
      ],
      "metadata": {
        "id": "z3uiiZRYmiCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the upstack using an upsampling template.\n",
        "\n",
        "!pip install -q git+https://github.com/tensorflow/examples.git"
      ],
      "metadata": {
        "id": "NjAWTP5zmnZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow_examples.models.pix2pix import pix2pix\n",
        "\n",
        "# Four upstack blocks for upsampling sizes \n",
        " # 4->8, 8->16, 16->32, 32->64 \n",
        "upstack = [pix2pix.upsample(512,3),\n",
        "          pix2pix.upsample(256,3),\n",
        "          pix2pix.upsample(128,3),\n",
        "          pix2pix.upsample(64,3)] "
      ],
      "metadata": {
        "id": "fazTHy6ancR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "upstack[0].layers"
      ],
      "metadata": {
        "id": "5kiqVlbjneEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Integrate the Segmentation Model"
      ],
      "metadata": {
        "id": "Yw4iiTfOngYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a U-Net model by merging downstack and upstack with skip-connections.\n",
        "\n",
        "# define the input layer\n",
        "inputs = keras.layers.Input(shape=[128,128,3])\n",
        "# downsample \n",
        "down = downstack(inputs)\n",
        "out = down[-1]\n",
        "# prepare skip-connections\n",
        "skips = reversed(down[:-1])\n",
        "# choose the last layer at first 4 --> 8\n",
        "# upsample with skip-connections\n",
        "for up, skip in zip(upstack,skips):\n",
        "    out = up(out)\n",
        "    out = keras.layers.Concatenate()([out,skip])\n",
        "# define the final transpose conv layer\n",
        "# image 128 by 128 with 56 classes\n",
        "out = keras.layers.Conv2DTranspose(56, 3,\n",
        "                                  strides=2,\n",
        "                                  padding='same',\n",
        "                                  )(out)"
      ],
      "metadata": {
        "id": "sY8hUfjXnjYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# complete UNet model\n",
        "unet = keras.Model(inputs=inputs, outputs=out)"
      ],
      "metadata": {
        "id": "9qWf0l-8nk5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the UNet model\n",
        "keras.utils.plot_model(unet, show_shapes=True)"
      ],
      "metadata": {
        "id": "FHomn0X0nlTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Preprocess the Data"
      ],
      "metadata": {
        "id": "WGcacdLunozJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resizing the images\n",
        "def resize_image(image):\n",
        "     image = tf.cast(image, tf.float32)\n",
        "     # scale values to [0,1]\n",
        "     image = image/255.0\n",
        "     # resize image\n",
        "     image = tf.image.resize(image, (128,128))\n",
        "     return image "
      ],
      "metadata": {
        "id": "BvHQDYPMnriz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resizing the annotations\n",
        "def resize_annotations(annotations):\n",
        "     annotations = tf.expand_dims(annotations, axis=-1)\n",
        "     annotations = tf.image.resize(annotations, (128,128))\n",
        "     annotations = tf.cast(annotations, tf.uint8)\n",
        "     return annotations "
      ],
      "metadata": {
        "id": "4Peh5mWVnswC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting to train_test\n",
        "X = [resize_image(i) for i in images]\n",
        "y = [resize_annotations(m) for m in annotations]"
      ],
      "metadata": {
        "id": "0MngRz0InuFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_X, val_X,train_y, val_y = train_test_split(X,y, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "OZMkgCO6nvY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = tf.data.Dataset.from_tensor_slices(train_X)\n",
        "val_X = tf.data.Dataset.from_tensor_slices(val_X)\n",
        "train_y = tf.data.Dataset.from_tensor_slices(train_y)\n",
        "val_y = tf.data.Dataset.from_tensor_slices(val_y)\n",
        "train_X.element_spec, train_y.element_spec, val_X.element_spec, val_y.element_spec "
      ],
      "metadata": {
        "id": "xC12LcYZnxAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = tf.data.Dataset.zip((train_X, train_y))\n",
        "val = tf.data.Dataset.zip((val_X, val_y))"
      ],
      "metadata": {
        "id": "6ltPIa6nnzS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Preparing the data batches and shuffling the train data"
      ],
      "metadata": {
        "id": "sdDvvMy1n324"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH = 64\n",
        "AT = tf.data.AUTOTUNE\n",
        "BUFFER = 1000\n",
        "STEPS_PER_EPOCH = 800//BATCH\n",
        "VALIDATION_STEPS = 200//BATCH\n",
        "train = train.cache().shuffle(BUFFER).batch(BATCH).repeat()\n",
        "train = train.prefetch(buffer_size=AT)\n",
        "val = val.batch(BATCH)"
      ],
      "metadata": {
        "id": "XLYsnLSgn1rG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Train the model"
      ],
      "metadata": {
        "id": "CZe4eDBfo84y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = next(iter(train))\n",
        "preds = unet(example[0])\n",
        "plt.imshow(example[0][60])\n",
        "plt.colorbar()\n",
        "plt.show() "
      ],
      "metadata": {
        "id": "MMwB6NLxn7Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Checking the untrained results"
      ],
      "metadata": {
        "id": "PgVc34aToHLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_annotations = tf.argmax(preds, axis=-1)\n",
        "pred_annotations = tf.expand_dims(pred_annotations, -1)\n",
        "plt.imshow(pred_annotations[0])\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "Qyo_zpI_n8pY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Compiling the model by optimizing the hyperparameter and training it for 50 epochs."
      ],
      "metadata": {
        "id": "2e2rSHJ_o4sv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unet.compile(loss= keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "             optimizer=keras.optimizers.RMSprop(lr=0.001),\n",
        "             metrics=['accuracy']) \n",
        "\n",
        "hist = unet.fit(train,\n",
        "              validation_data=val,\n",
        "              steps_per_epoch=STEPS_PER_EPOCH,\n",
        "              validation_steps=VALIDATION_STEPS,\n",
        "              epochs=50)"
      ],
      "metadata": {
        "id": "x5BgHUQSn-ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Performance Evaluation"
      ],
      "metadata": {
        "id": "ZmtV7ieKoEND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see our model performance\n",
        "\n",
        "img, mask = next(iter(val))\n",
        " pred = unet.predict(img)\n",
        "\n",
        " plt.figure(figsize=(10,5))\n",
        " for i in pred:\n",
        "     plt.subplot(121)\n",
        "     i = tf.argmax(i, axis=-1)\n",
        "     plt.imshow(i,cmap='jet')\n",
        "     plt.axis('off')\n",
        "     plt.title('Prediction')\n",
        "     break\n",
        "\n",
        " plt.subplot(122)\n",
        " plt.imshow(mask[0], cmap='jet')\n",
        " plt.axis('off')\n",
        " plt.title('Ground Truth')\n",
        " plt.show() "
      ],
      "metadata": {
        "id": "TQqKSmpGoALZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = hist.history\n",
        "acc=history['accuracy']\n",
        "val_acc = history['val_accuracy']\n",
        "plt.plot(acc, '-', label='Training Accuracy')\n",
        "plt.plot(val_acc, '--', label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bLoevlZkoBjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Description of the semantic segmentation network\n",
        "\n",
        "##### Semantic Segmentation\n",
        "Semantic Segmentation laid down the fundamental path to advanced Computer Vision tasks such as object detection, shape recognition, autonomous driving, robotics, and virtual reality. Semantic segmentation can be defined as the process of pixel-level image classification into two or more Object classes. It differs from image classification entirely, as the latter performs image-level classification. For instance, consider an image that consists mainly of a zebra, surrounded by grass fields, a tree and a flying bird. Image classification tells us that the image belongs to the ‘zebra’ class. It can not tell where the zebra is or what its size or pose is. But, semantic segmentation of that image may tell that there is a zebra, grass field, a bird and a tree in the given image (classifies parts of an image into separate classes). And it tells us which pixels in the image belong to which class. Here we will discuss semantic segmentation using TensorFlow Keras.\n",
        "\n",
        "We have to build a computer vision model that can convert an input image into a segmented image (also called masked image or label image).\n",
        "\n",
        "By understanding how semantic segmentation works, we can easily come up with an idea of how to choose our pre-trained model. One of the popular architectural approaches is FCNN (Fully Convolutional Neural Networks). In contrast to CNNs in image classification, where the decision head is made up of dense layers, an FCNN is made up of layers related to convolutional operations only. Because the final output is an image of a shape identical to the input image. \n",
        "\n",
        "An FCNN contains two parts: an encoder and a decoder. An encoder is a downstack of convolutional neural layers that extract features from the input image. A decoder is an upstack of transpose convolutional neural layers that builds the segmented image from the extracted features. The sizes of feature maps go down while downsampling (e.g. 128, 64, 32, 16, 8, 4 – in order), and they go up while upsampling (e.g. 4, 8, 16, 32, 64, 128 – in order).\n",
        "\n",
        "Among FCNNs, U-Net is one of the successful architectures acclaimed for its performance in Medical Image Segmentation. It encourages skip connections between a few specific-sized layers of downstack and upstack. Skip-connections yield better performance because of the truth that upstack struggles to build finer details of the image on its own during upsampling. Skip-connections bye-pass a large stack of layers to feed finer details from a downstack layer to its corresponding upstack layer.\n",
        "\n",
        "#### U-Net model\n",
        "\n",
        "Here, we wish to use the functional approach of U-Net architecture, but we will have our own architecture suitable to our task. The downstack can be a pre-trained CNN, trained for image classification (e.g. MobileNetV2, ResNet, NASNet, Inception, DenseNet, or EfficientNet). It can effectively extract the features. But, we have to build our upstack to match our classes (here, 59), build skip-connections, and train it with our data. \n",
        "\n",
        "We prefer a pre-trained DenseNet121 to be the downstack that can be obtained through transfer learning and build the upstack with pix2pix, a publicly available generative upstack template. We build the downstack with the above layers. We use the pre-trained model as such, without any fine-tuning and build a U-Net model by merging downstack and upstack with skip-connections.\n",
        "\n",
        "We select the final ReLU activation layer for each feature map size, i.e. 4, 8, 16, 32, and 64, required for skip-connections.\n",
        "\n",
        "The amount of training data plays a vital role in performance in a deep learning model. Limited data, in our case, maybe one reason for relatively poor performance. By tuning hyperparameters carefully, improving or changing model architectures, and increasing training data, we could achieve performance improvements.\n",
        "\n",
        "We observe closeness to a certain level. By training for more epochs, we can obtain improvement in the results."
      ],
      "metadata": {
        "id": "kiwI1hzccYCJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Milestone 5: Documentation\n",
        "\n",
        "Below is the code to convert a notebook into an HTML file"
      ],
      "metadata": {
        "id": "WDk4zQijfDXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nbconvert"
      ],
      "metadata": {
        "id": "EnEJkvU_fiHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "jupyter nbconvert --to html /content/Milestone_5_Documentation.ipynb\n",
        "# replace the path above with the path to your notebook"
      ],
      "metadata": {
        "id": "dY4fOOGhfSEJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}